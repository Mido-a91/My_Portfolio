{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reason_1</th>\n",
       "      <th>Reason_2</th>\n",
       "      <th>Reason_3</th>\n",
       "      <th>Reason_4</th>\n",
       "      <th>Month Value</th>\n",
       "      <th>Day of the Week</th>\n",
       "      <th>Transportation Expense</th>\n",
       "      <th>Distance to Work</th>\n",
       "      <th>Age</th>\n",
       "      <th>Daily Work Load Average</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Education</th>\n",
       "      <th>Children</th>\n",
       "      <th>Pets</th>\n",
       "      <th>Absenteeism Time in Hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>225</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>330</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Reason_1  Reason_2  Reason_3  Reason_4  Month Value  Day of the Week  \\\n",
       "0           0         0         0         1            7                1   \n",
       "1           0         0         0         0            7                1   \n",
       "2           0         0         0         1            7                2   \n",
       "3           1         0         0         1            7                3   \n",
       "4           0         0         0         1            7                3   \n",
       "..        ...       ...       ...       ...          ...              ...   \n",
       "695         1         0         0         1            5                2   \n",
       "696         1         0         0         1            5                2   \n",
       "697         1         0         0         1            5                3   \n",
       "698         0         0         0         1            5                3   \n",
       "699         0         0         0         1            5                3   \n",
       "\n",
       "     Transportation Expense  Distance to Work  Age  Daily Work Load Average  \\\n",
       "0                       289                36   33                  239.554   \n",
       "1                       118                13   50                  239.554   \n",
       "2                       179                51   38                  239.554   \n",
       "3                       279                 5   39                  239.554   \n",
       "4                       289                36   33                  239.554   \n",
       "..                      ...               ...  ...                      ...   \n",
       "695                     179                22   40                  237.656   \n",
       "696                     225                26   28                  237.656   \n",
       "697                     330                16   28                  237.656   \n",
       "698                     235                16   32                  237.656   \n",
       "699                     291                31   40                  237.656   \n",
       "\n",
       "     Body Mass Index  Education  Children  Pets  Absenteeism Time in Hours  \n",
       "0                 30          0         2     1                          4  \n",
       "1                 31          0         1     0                          0  \n",
       "2                 31          0         0     0                          2  \n",
       "3                 24          0         2     0                          4  \n",
       "4                 30          0         2     1                          2  \n",
       "..               ...        ...       ...   ...                        ...  \n",
       "695               22          1         2     0                          8  \n",
       "696               24          0         1     2                          3  \n",
       "697               25          1         0     0                          8  \n",
       "698               25          1         0     0                          2  \n",
       "699               25          0         1     1                          2  \n",
       "\n",
       "[700 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed = pd.read_csv('Absenteeism-Preprocessed-My_trial.csv')\n",
    "data_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use Logistic Regression to Predict Absenteesim\n",
    "- A type of classification\n",
    "- So what are the classes - Decide what classes are there first and then preprocess the data to reflect the decision  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to know the Median of the hours\n",
    "- Using statistics we know that this will implicity balance the dataset\n",
    "- This will also make us have equal priors of 0s and 1s, so the model doesnt bias one target over the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absenteesim_median = data_preprocessed['Absenteeism Time in Hours'].median()\n",
    "absenteesim_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,   0,   2,   8,  40,   1,   7,   3,  32,   5,  16,  24,  64,\n",
       "        56,  80, 120, 112, 104,  48], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed['Absenteeism Time in Hours'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) More PreProcessing - Absent hours to Binary data of 0 and 1\n",
    "- There are 19 unique different hours for absent hours\n",
    "- We can use .map with a dictionary for each hour and say these are 0 and these are 1 .map({})\n",
    "- but here since they are alot we can map them into binary using the where the method\n",
    "- This makes go from pandas series to numpy pandas series again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.where(data_preprocessed['Absenteeism Time in Hours']>absenteesim_median,1,0)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed['Targets'] = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reason_1</th>\n",
       "      <th>Reason_2</th>\n",
       "      <th>Reason_3</th>\n",
       "      <th>Reason_4</th>\n",
       "      <th>Month Value</th>\n",
       "      <th>Day of the Week</th>\n",
       "      <th>Transportation Expense</th>\n",
       "      <th>Distance to Work</th>\n",
       "      <th>Age</th>\n",
       "      <th>Daily Work Load Average</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Education</th>\n",
       "      <th>Children</th>\n",
       "      <th>Pets</th>\n",
       "      <th>Absenteeism Time in Hours</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>225</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>330</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Reason_1  Reason_2  Reason_3  Reason_4  Month Value  Day of the Week  \\\n",
       "0           0         0         0         1            7                1   \n",
       "1           0         0         0         0            7                1   \n",
       "2           0         0         0         1            7                2   \n",
       "3           1         0         0         1            7                3   \n",
       "4           0         0         0         1            7                3   \n",
       "..        ...       ...       ...       ...          ...              ...   \n",
       "695         1         0         0         1            5                2   \n",
       "696         1         0         0         1            5                2   \n",
       "697         1         0         0         1            5                3   \n",
       "698         0         0         0         1            5                3   \n",
       "699         0         0         0         1            5                3   \n",
       "\n",
       "     Transportation Expense  Distance to Work  Age  Daily Work Load Average  \\\n",
       "0                       289                36   33                  239.554   \n",
       "1                       118                13   50                  239.554   \n",
       "2                       179                51   38                  239.554   \n",
       "3                       279                 5   39                  239.554   \n",
       "4                       289                36   33                  239.554   \n",
       "..                      ...               ...  ...                      ...   \n",
       "695                     179                22   40                  237.656   \n",
       "696                     225                26   28                  237.656   \n",
       "697                     330                16   28                  237.656   \n",
       "698                     235                16   32                  237.656   \n",
       "699                     291                31   40                  237.656   \n",
       "\n",
       "     Body Mass Index  Education  Children  Pets  Absenteeism Time in Hours  \\\n",
       "0                 30          0         2     1                          4   \n",
       "1                 31          0         1     0                          0   \n",
       "2                 31          0         0     0                          2   \n",
       "3                 24          0         2     0                          4   \n",
       "4                 30          0         2     1                          2   \n",
       "..               ...        ...       ...   ...                        ...   \n",
       "695               22          1         2     0                          8   \n",
       "696               24          0         1     2                          3   \n",
       "697               25          1         0     0                          8   \n",
       "698               25          1         0     0                          2   \n",
       "699               25          0         1     1                          2   \n",
       "\n",
       "     Targets  \n",
       "0          1  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  \n",
       "..       ...  \n",
       "695        1  \n",
       "696        0  \n",
       "697        1  \n",
       "698        0  \n",
       "699        0  \n",
       "\n",
       "[700 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BALANCING Targets - Checking the Priors - Already taken care of while converting to binary\n",
    "- usualy we want 50 50\n",
    "- but 45% 55% is still sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45571428571428574"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed['Targets'].sum()/data_preprocessed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0 % are 1s\n"
     ]
    }
   ],
   "source": [
    "print(np.round(data_preprocessed['Targets'].sum()/data_preprocessed.shape[0]*100),'% are 1s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets drop the Unnecessary Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Creating Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_targets is data_preprocessed\n",
    "# this to confirm that the two data frames are not the same - the answer is boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reason_1</th>\n",
       "      <th>Reason_2</th>\n",
       "      <th>Reason_3</th>\n",
       "      <th>Reason_4</th>\n",
       "      <th>Month Value</th>\n",
       "      <th>Day of the Week</th>\n",
       "      <th>Transportation Expense</th>\n",
       "      <th>Distance to Work</th>\n",
       "      <th>Age</th>\n",
       "      <th>Daily Work Load Average</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Education</th>\n",
       "      <th>Children</th>\n",
       "      <th>Pets</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>225</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>330</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Reason_1  Reason_2  Reason_3  Reason_4  Month Value  Day of the Week  \\\n",
       "0           0         0         0         1            7                1   \n",
       "1           0         0         0         0            7                1   \n",
       "2           0         0         0         1            7                2   \n",
       "3           1         0         0         1            7                3   \n",
       "4           0         0         0         1            7                3   \n",
       "..        ...       ...       ...       ...          ...              ...   \n",
       "695         1         0         0         1            5                2   \n",
       "696         1         0         0         1            5                2   \n",
       "697         1         0         0         1            5                3   \n",
       "698         0         0         0         1            5                3   \n",
       "699         0         0         0         1            5                3   \n",
       "\n",
       "     Transportation Expense  Distance to Work  Age  Daily Work Load Average  \\\n",
       "0                       289                36   33                  239.554   \n",
       "1                       118                13   50                  239.554   \n",
       "2                       179                51   38                  239.554   \n",
       "3                       279                 5   39                  239.554   \n",
       "4                       289                36   33                  239.554   \n",
       "..                      ...               ...  ...                      ...   \n",
       "695                     179                22   40                  237.656   \n",
       "696                     225                26   28                  237.656   \n",
       "697                     330                16   28                  237.656   \n",
       "698                     235                16   32                  237.656   \n",
       "699                     291                31   40                  237.656   \n",
       "\n",
       "     Body Mass Index  Education  Children  Pets  Targets  \n",
       "0                 30          0         2     1        1  \n",
       "1                 31          0         1     0        0  \n",
       "2                 31          0         0     0        0  \n",
       "3                 24          0         2     0        1  \n",
       "4                 30          0         2     1        0  \n",
       "..               ...        ...       ...   ...      ...  \n",
       "695               22          1         2     0        1  \n",
       "696               24          0         1     2        0  \n",
       "697               25          1         0     0        1  \n",
       "698               25          1         0     0        0  \n",
       "699               25          0         1     1        0  \n",
       "\n",
       "[700 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Selecting inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reason_1</th>\n",
       "      <th>Reason_2</th>\n",
       "      <th>Reason_3</th>\n",
       "      <th>Reason_4</th>\n",
       "      <th>Month Value</th>\n",
       "      <th>Day of the Week</th>\n",
       "      <th>Transportation Expense</th>\n",
       "      <th>Distance to Work</th>\n",
       "      <th>Age</th>\n",
       "      <th>Daily Work Load Average</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Education</th>\n",
       "      <th>Children</th>\n",
       "      <th>Pets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>225</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>330</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>237.656</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Reason_1  Reason_2  Reason_3  Reason_4  Month Value  Day of the Week  \\\n",
       "0           0         0         0         1            7                1   \n",
       "1           0         0         0         0            7                1   \n",
       "2           0         0         0         1            7                2   \n",
       "3           1         0         0         1            7                3   \n",
       "4           0         0         0         1            7                3   \n",
       "..        ...       ...       ...       ...          ...              ...   \n",
       "695         1         0         0         1            5                2   \n",
       "696         1         0         0         1            5                2   \n",
       "697         1         0         0         1            5                3   \n",
       "698         0         0         0         1            5                3   \n",
       "699         0         0         0         1            5                3   \n",
       "\n",
       "     Transportation Expense  Distance to Work  Age  Daily Work Load Average  \\\n",
       "0                       289                36   33                  239.554   \n",
       "1                       118                13   50                  239.554   \n",
       "2                       179                51   38                  239.554   \n",
       "3                       279                 5   39                  239.554   \n",
       "4                       289                36   33                  239.554   \n",
       "..                      ...               ...  ...                      ...   \n",
       "695                     179                22   40                  237.656   \n",
       "696                     225                26   28                  237.656   \n",
       "697                     330                16   28                  237.656   \n",
       "698                     235                16   32                  237.656   \n",
       "699                     291                31   40                  237.656   \n",
       "\n",
       "     Body Mass Index  Education  Children  Pets  \n",
       "0                 30          0         2     1  \n",
       "1                 31          0         1     0  \n",
       "2                 31          0         0     0  \n",
       "3                 24          0         2     0  \n",
       "4                 30          0         2     1  \n",
       "..               ...        ...       ...   ...  \n",
       "695               22          1         2     0  \n",
       "696               24          0         1     2  \n",
       "697               25          1         0     0  \n",
       "698               25          1         0     0  \n",
       "699               25          0         1     1  \n",
       "\n",
       "[700 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_inputs = data_with_targets.iloc[:,0:-1]\n",
    "unscaled_inputs\n",
    "\n",
    "# or data_with_targets.iloc[:,:-1]\n",
    "# or data_with_targets.iloc[:,0:14]\n",
    "# data_with_targets.iloc[:,:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Standardizing All Inputs - Feature Scaling - EXCEPT THE BINARY/DUMMIES\n",
    "##### STANDARDIZATION ALWAYS YIELDS HIGHER ACCURACY BUT SACRIFICES INTERPRETABILITY - SO DEPENDING ON WHAT WE NEED WE MIGHT DO OR DONT DO STANDARDIZTION - Because Optimization algorithm works better with equal magnitutes\n",
    "- Here we will use a custom scaler class, that will wil scale all features EXCEPT the binary and the dummies features (we will tell it which columns to exclude)\n",
    "- Both preprocessing.StandardScaler() and preprocessing.scale() do the exact same thing: scale data to have a mean of 0 and a standard deviation of 1. \n",
    "- However, preprocessing.scale() is just a function\n",
    "- preprocessing.StandardScaler(), as stated in the official docs, is a class that implements the Transformer API to compute the mean and standard deviation on a training set, so as to be able to later reapply the same transformation on the testing set. - - Therefore, using preprocessing.StandardScaler() is more recommended.\n",
    "\n",
    "- mnist_train.map(scale) was just used for tf.data data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Side Note - Standardization for different Roles\n",
    "- Machine Learning Engineers: Prefer Models with standardized variables to achieve higher accuracy via the optimization algorithm which prefers the standardized variables\n",
    "- Statisticians and Econometricians: Prefer less accuracy so working with non standardized features, because they care about the underlying reasons behind different phenomena\n",
    "- Data Scientists: Either Position; Higher accuracy (Stand Features) or Lower Acc (Non Stand Features) -- But the norm is standardizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we were to use a regular scaler - Standardizing all Features:\n",
    "- Import\n",
    "- CREATE a standard scaler object with no info in it (an instance of standard scaler)\n",
    "    - This object will be used to subtract the mean and divide by the std deviation variablewise (feature wise)\n",
    "- FIT the standard scaler object (at the features) -- will extract the mean and variance\n",
    "- TRANSFORM the features (actually does the standardization by subtracting the mean and dividing the variance)\n",
    "    - Transform does the actual scaling using the info saved when fitted the absenteeism scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "absenteeism_scaler = StandardScaler()\n",
    "\n",
    "absenteeism_scaler.fit(unscaled_inputs)\n",
    "\n",
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a custom scaler -- A Tool to have \n",
    "- We need to NOT standardize the binary predictors -- LEAVE THEM 0s and 1s\n",
    "- We need the binary/dummies to maintain their interpretability (ex ODDS Ratio)\n",
    "    - ex Reason 3 is 16 times more likely to be absent than its base refrence Reason 0 which has no reason to be absent \n",
    "- We still have predictive power but in this case we dont know how the different reasons compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Its a custom made scaler based on the standard scaler from SKLEARN with added property of choosing which columns to NOT scale\n",
    "- You dont need to memorize or worry about the class - Just know how to use it\n",
    "- In practice we can also standardize PRIOR to creating dummies \n",
    "- The only difference to notice when we are declaring the object custom scaler - There is an extra argument we will outline all features EXCEPT binary/dummies \n",
    "- We wont lose much in the accuracy - But we win interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries needed to create the Custom Scaler\n",
    "# note that all of them are a part of the sklearn package\n",
    "# moreover, one of them is actually the StandardScaler module, \n",
    "# so you can imagine that the Custom Scaler is build on it\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the Custom Scaler class\n",
    "\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    # and what is calculated/declared as we do\n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        \n",
    "        # scaler is nothing but a Standard Scaler object\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        # with some columns 'twist'\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        \n",
    "    \n",
    "    # the fit method, which, again based on StandardScale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "        \n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        \n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        \n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "        # use the original order (that you recorded in the beginning)\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Scale all but Binary/Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month Value',\n",
       "       'Day of the Week', 'Transportation Expense', 'Distance to Work',\n",
       "       'Age', 'Daily Work Load Average', 'Body Mass Index', 'Education',\n",
       "       'Children', 'Pets'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_inputs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass copy=True, with_mean=True, with_std=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "columns_to_scale = ['Month',\n",
    "       'Day of The Week', 'Transportation Expense', 'Distance to Work',\n",
    "       'Age', 'Daily Work Load Average', 'Body Mass Index', \n",
    "       'Children', 'Pets']\n",
    "\n",
    "absenteeism_scaler = CustomScaler(columns_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fitting the scaler with the inputs  - Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Month', 'Day of The Week'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e06df2df7bf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabsenteeism_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munscaled_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# now we are fitting unscaled inputs -- SPECIFICALLY AND ONLY THE columns_to_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Standardization information is now saved in this object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# This object is not empty anymore. Now it contains info about the mean and std deviation of the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4f9146ac1405>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2984\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2986\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Month', 'Day of The Week'] not in index\""
     ]
    }
   ],
   "source": [
    "absenteeism_scaler.fit(unscaled_inputs)\n",
    "\n",
    "# now we are fitting unscaled inputs -- SPECIFICALLY AND ONLY THE columns_to_scale\n",
    "# Standardization information is now saved in this object\n",
    "# This object is not empty anymore. Now it contains info about the mean and std deviation of the inputs\n",
    "# Can be used to standardize new data using the same mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) To apply it/activate: Transform - Same as before\n",
    "- Transform does the actual scaling using the info saved when fitted the absenteeism scaler\n",
    "- basically we are actually subtracting the mean and dividing the std dev\n",
    "- now new_data_scaled = absenteeism_scaler.transform(unscaled_new_data)\n",
    "    - This will scale the new data using the same mean and std dev of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)\n",
    "scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train Test Split - Conclusion - (comes with shuffle - as a deal :P)\n",
    "- IT IS THE LAST STEP OF PREPROCESSING AFTER DIVIDING INTO INPUTS AND TARGETS\n",
    "- We split to prevent overfitting, and not letting the model see all of the data until we verify or test\n",
    "- Using the Method: In linear regression we divided data into inputs targets and we just used train_test_split. Gives 4 outputs: train input test input train target test target\n",
    "- Manually (for DeepLearning): In MNIST  our dataset was imported as an iterative tuple already containing inputs and targets, and already split to Train and Test. We need to split Train to Train and Validation and that was done manually using a percentage of the number of the observations. Thats why we couldnt use the method of train and test only\n",
    "- Manually (for DeepLearning): In Business case, we divided the data into inputs and targets. Then we needed to split the data into Train, Validation and Test manually using number of observations. Thats why we couldnt use the method of train and test only\n",
    "- Using the Method: In Logistic Regression we follow what we did in linear regression and use the method. Having divided the data into inputs and targets. Gives 4 outputs: train input test input train target test target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Shuffling: Now if we use the split Train Test. It comes with a shuffle parameter \n",
    "- We want to shuffle to remove all types of dependencies that come from the dataset ex:day of the week\n",
    "- The randomness (shuffle) will be applied by default, howver we need to shuffle the observations in the same random way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(scaled_inputs,targets, train_size=0.8,random_state=20)\n",
    "\n",
    "# x_train,x_test,y_train,y_test = train_test_split(scaled_inputs,targets, test_size=0.2,random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train test to 80 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of Train data is: \",x_train.shape[0]/scaled_inputs.shape[0]*100,'%', \"Percentage of Test data is: \",x_test.shape[0]/scaled_inputs.shape[0]*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Side note: Preprocessing Conclusion\n",
    "\n",
    "### 1) Preprocessing Before Splitting inputs/outputs:\n",
    "- 1) DISPLAY max rows and columns\n",
    "- 2) CHECKNA for nulls/drop nulls from rows/reset index\n",
    "- 3) DROP unneccessary columns/change names of cols/change their order\n",
    "- 4) DUMMIES Deal with categorical variables/dummies(Group?)/binaries(map?)(using where?)\n",
    "- 5) OUTLIERS Deal with outliers (like we did in linear regression) using distribution curves and quantiles/percentiles\n",
    "- 6) TARGETS Deal with target variable (binary?)(median?)(balance priors?)for Logistic\n",
    "\n",
    "\n",
    "##### SPLIT DATA INTO INPUTS TARGETS\n",
    "\n",
    "\n",
    "### 2) Preprocessing After Splitting inputs/outputs:\n",
    "- 1) STANDARDIZE to have equal magnitudes (feature scaling of inputs)\n",
    "- 2) BALANCE the priors of targets to have non biased model outputs - in AudioBooks we already had 0 and 1 so we cut from it. in Absenteeism we balanced the 0 and 1 while doing the binary conversion (using the median)\n",
    "- 3) SHUFFLE the data set to remove dependencies (if you are not going to use the split train test then shuffle manually) -- Otherwise its included in Train Test Split\n",
    "- 4) SPLIT the inputs targets to take care of overfitting (manually or using the method) to Train Test or Train Validation Test\n",
    "  - train_test method contains shuffle\n",
    "  - otherwise if doing the split manually then shuffle done before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) The Logistic Regression Model\n",
    "- Machine learning is 90% Preprocessing an 10% actual modeling\n",
    "- Previously we did logistic Regression using StatsModels. However in this particular time LR from sm was unstable.\n",
    "- We will be using sklearn for Logistic Regression for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a logistic regression object\n",
    "\n",
    "reg = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To evaluate the Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Checking Accuracy Manually\n",
    "- Model learned to classify (predict) 80% of the observations correctly\n",
    "- Its good to understand what happens in the background\n",
    "- will be used later\n",
    "- We need to find the predictions (output) and compare to targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_outputs = reg.predict(x_train)\n",
    "model_outputs\n",
    "\n",
    "# predictions of the trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs is y_train\n",
    "\n",
    "# is compares the arrays as a whole (they are not the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_outputs == y_train\n",
    "\n",
    "# compares them element by element\n",
    "# True ==1\n",
    "# False ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model_outputs == y_train)\n",
    "# total number of true entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs.shape[0]\n",
    "# total number of all entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking for accuracy - Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model_outputs == y_train)/model_outputs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** Side Note StatsModels Vs Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### University Admittance Problem: We used STATSMODELS -- \n",
    "- Previously in Logistic Regression of the addmitance based on SAT and Gender. \n",
    "- .pred() >> Our output/Predictions are results of 0s and 1s and everything in between(ex 0.3, 0.8 etc). So we used the confusion matrix to check for accuracy.\n",
    "- .pred_table() provides the confusion matrix\n",
    "\n",
    "#### Absenteeism Problem: In This example we use SKLEARN --  \n",
    "- Here however the outputs are perfectly 0s and 1s so we used the '==' boolean operator instead to check for accuracy against the targets of 0s and 1s\n",
    "- .predict() SKLEARN automatically converts the 0 1s and inbetween (0.3 to 0 and 0.6 to 1) to 0s and 1s\n",
    "- .predict_proba() provides the probability of being 0 (first column) and probability of being 1 (second column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Coefficients and Intercept\n",
    "- We apply the coefficients to the inputs to obtain final results (predictions/outputs)\n",
    "- So to use the model (logistic regression) outside, We need to understand the coefficients and intercepts of the model so we can use them outside python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_inputs.columns.values\n",
    "# wouldnt work since the scaled inputs is an array and columns values works with pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Side note - Output of Sklearn vs Statsmodels\n",
    "- Statsmodels outputs and deals with dataframes\n",
    "- However Sklearn outputs everything into numpy arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name= unscaled_inputs.columns.values\n",
    "feature_name\n",
    "# 1 dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_\n",
    "# 2 dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Creating summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table=pd.DataFrame(data=feature_name,columns=['Features'])\n",
    "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
    "\n",
    "# if you want to rename the indices:\n",
    "# weights.index = feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the intercept to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_table.index=summary_table.index+1\n",
    "summary_table\n",
    "\n",
    "# we added a 1 to each index from 0 to 13 now 1 to 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to change the type of reg.intercept numpyfloat 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reg.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reg.intercept_[0].item())\n",
    "\n",
    "# .item() converts numpy data types into python data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = reg.intercept_[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We slice and assign a list to that slice\n",
    "- Before we add the content of the intercept_[0]. It has a numpy float 64 type\n",
    "- We use .item() to convert the numpy type to native python type\n",
    " - numpyfloat64 >> float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we assign a list to the slice at row 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_table.loc[0]=['Intercept',intercept]\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Side Note: Sort in Numpy vs Pandas\n",
    "- Pandas\n",
    "    - 1) df.sort_values(by=['col1'])\n",
    "    - 2) sorted(df)\n",
    "\n",
    "- NUMPY: np.sort(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Side Note: Transposing/Converting Matrices to Vectors\n",
    "- Transpose only transposes the matrix (np arrays-rows to columns) if it was just 1 row matrix >> 1 col vector which is fine to be added into a dataframe\n",
    "- reshape converts a matrix into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.T\n",
    "\n",
    "# np.transpose(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.T.shape\n",
    "\n",
    "# Result is a 1 dimensional Matrix which is a vertical vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### reshape can be used to convert 2 to 1 and 1 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.reshape(-1,).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_.reshape(-1,1).shape\n",
    "# Result is a 1 dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Side note remembering DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=[[1,2,3],[4,5,6],[6,7,8]],columns=['1','2','3'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** Side Note: Putting Data into a Dataframe\n",
    "- IF we have columns of data\n",
    "    - 1) you can add one column at a time using data= and coloumn=\n",
    "    - 2) or use a dictionary of multiple columns at the same time and their data\n",
    "- If we have rows of data you can input them as below, will be entered horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** SIDE NOTE - putting an array into a data Frame\n",
    "- numpy arrays by default are horizontal vectors or horizontal matrices or horizontals tensors\n",
    "- There are two ways to put an array into a dataframe series: as Vertical or Horizontal Vectors\n",
    "    - 1) if its a horizontal 2D dimensional array (a matrix of 1 row)\n",
    "        - A) You can transpose it  >> Becomes a vertical Vector (a matrix of 1 column) \n",
    "        - B) Reshape it into a vector reshape(-1,1) max rows 1 colum >> Becomes a vertical vector (a matrix of 1 column) \n",
    "        - C) Reshape it into a vector reshape(-1) max rows >> Becomes a horizontal vector\n",
    "    - 2) if we have a horizontal 1D dimensional array, you can just add it to the dataframe (horizontal vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Interpretation of the coefficients and intercept and Odds Ratio \n",
    "- Coefficients/Weights: How we weigh a certain input\n",
    "    - Standardizing the features standardizes the Weights (variables have a variance of 1 - MEANS SAME SCALE)\n",
    "    - Once we have scaled features, The bigger the weight than 0, the bigger it is and the more important its feature is and vice versa) \n",
    "- ODDS RATIO (FOR LOGISTIC ONLY) basically tells you how much more likely the feature is to contribute the total output likelihood, than its base model reference - the one equal to zero (ex: male,female binary = 0,1) -- only for the non standardized features\n",
    "- Bias is intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Via Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_table['Odds_ratio'] = np.exp(summary_table['Coefficients'])\n",
    "\n",
    "summary_table = summary_table.sort_values(by='Odds_ratio',ascending = False)\n",
    "summary_table\n",
    "# from most to least important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Weights: \n",
    "- FEATURE IS NOT IMPORTANT\n",
    "  - if the coefficient is around 0 == A weight of zero multiplied by input becomes zero\n",
    "  \n",
    "#### Interpretation of ODDS RATIO: \n",
    "- Odds ratio wouldnt have a meaning or interpretability for the features that are standardized - THATS WHY WE DONT STANDARDIZE THE BINARY/DUMMIES - Because we want to compare them to their base Model\n",
    "- ODDS RATIO basically tells you how much bigger the feature is than its base reference model - the one equal to zero (ex: male,female binary = 0,1) in the binary case\n",
    "- FEATURE IS NOT IMPORTANT if its odds ratio is around 1 == when odds ratio = 1, odds dont change at all - your feature is 1 times bigger than the base reference no value\n",
    "  - For a unit change of in the standardized feature, the odds increase by a multiple equal to the odds ratio (but 1 = no change)\n",
    "  - for example ODDS X ODDS RATIO = NEW ODDS \n",
    "  - 5:1 X 2 = 10:1 (unit change is 2) \n",
    "  - 5:1 X 0.2 = 1:1 (unit change is 0.2)\n",
    "  - 5:1 X 1 = 5:1 (unit change is 1) so odds ratio of 1 makes no change == weight= zero\n",
    "  - LOGISTIC in the ODDS ENVIROMENT ==> ODDS = ODDS RATIO X ODDS + ODDS RATIO X ODDS...\n",
    "  - LOGISTIC IN WEIGHTS ENVIROMENT ==> log(odds) = w1x1+w2x2....+b\n",
    "  \n",
    "### ** INSIGHTS - INTERPRETATION OF THE MODEL LOGISITIC COEFFICIENTS **\n",
    "### HIGHEST WEIGHTS/ODDS RATIO\n",
    "\n",
    "###### Non Standardized BINARY/DUMMY Predictors  - ODDS RATIO HAS MEANING\n",
    "- All the reason features are actually orginally 1 feature Reason for absense with a base reference of Reason 0 which we dropped - so the odds ratio will compare each of them to the base reference\n",
    "- REASON 1 2 3 4 - Remember we dropped a variable for multicollinearity that became base reference\n",
    "- That was reason 0 - A situation where a person was absent but no reason for it\n",
    "- Reason 1 = various disease, Reason 2 = Pregnancy Reason, 3 = Poisoning, Reason 4 Light Disease\n",
    "- Reason 3: Most significant with biggest weight and odds ratio is understandably Reason 3 = Poisoining is around 17 times more likely to be absent (output) than Reason 0 (base ref) with no reason to be absent\n",
    "- Reason 1: Second place is Reason 1 = Various diseases - Normal Absenteeism -- You got sick you were absent - 14 times more likely to be absent than no reason (reason 0)\n",
    "- Reason 4: Third place is various disease around 5 times more likely to be absent that no reason\n",
    "- Reason 2: last reason is pregnancy around 2 times more likely to be absent than no reason\n",
    "\n",
    "###### Standardized NON - BINARY/DUMMY Predictors - Features STANDARDIZED CANT COMPARE TO BASE MODEL ==0  -- doesnt have interpretatbility\n",
    "- POSTIVE INCREASE: Transportation Expense: ODDS RATIO = 2 -- This is a non Binary/Dummy Predictor\n",
    "    - We dont have a direct interpretability of it (Standardized)\n",
    "    - The way to interpret it - Is its ODDS ratio implies that for 1 standardized unit or (for 1 standard deviation increase) in the feature (Transportation Expense), the odds are twice as likely to be excessively absent than base model\n",
    "- NEGATIVE INCREASE: Pets: continous variable - ODDS RATIO = 0.75\n",
    "    - Its odds ratio implies that for each 1 standardized unit increase (1 std dev increase)(or 1 unit increase in the standardized feature) in the feature, the odds are 1-0.75 = 24% lower than the base model (no Pets)\n",
    "- Intercept: There is no meaning attached to it - IT CALIBRATES THE MODEL - Without the bias/intercept, each prediction would be off the mark by exactly that value\n",
    "\n",
    "### LOWEST WEIGHTS/ODDS RATIO\n",
    "- Daily work load & Day of the WEEK & Distance to work seem to have the lowest weights and ODDS ratio\n",
    "- OR we can say given all features these seem to be the ones that make little to no difference\n",
    "- Consider dropping them later on - (BACKWARD ELIMINATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION & LOG ODDS & ODDS RATIO\n",
    "- Logistic Regression is a Linear function predicting Log odds. These Log odds are later transformed into 0 and 1\n",
    "- log(odds) = w1x1+w2x2....+b\n",
    " - To get the odds we get the exponential of both sides \n",
    "     - Expo of the log cancels out == Odds\n",
    "     - We get exponential of the weights (Coefficients) ==> ODDS RATIO\n",
    " - We get ODDS = ODDS RATIO X ODDS + ODDS RATIO X ODDS...\n",
    " - Exponential of coefficients are called Odds Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary/Dummy Predictors in LOGISTIC REGRESSION (FEATURES)'\n",
    "- We can have binary/dummy variable in any type of regression. However the binary predictors in Logistic regression are important predictors\n",
    "- Basically in Logistic Regresson Binary/Dummy features or Predictors have ODDS ratio that we can explore\n",
    "- When we consider Binary converted variables or Dummy Variables (Features/predictors), they always must have 1 base reference (features)\n",
    "\t- 1) Binary: one of them is zero (ex male) that is the base reference\n",
    "\t- 2) Dummies: Drop 1 variable (for Multicollinearity) becomes the base reference\n",
    "- By looking at the features, their coefficients and their ODDS ratio. ODDS RATIO (FOR LOGISTIC ONLY) basically tells you how much more likely the feature is to contribute the total output likelihood, than its base model reference. For example odds ratio of type 1 reason of absence = 3. This means that odds reason 3 = 16 times odds reason 0 or (odds reason1/odds reason 0 == 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) BACKWARD ELIMINATION & Using List Comprehension - Once you reach this step take a look at copy 2\n",
    "- The results will be almost the same for the model and its coefficients - a slight difference in the accuracy\n",
    "- I created a another copy for it, since in reality we will go back and ajust the code, and we wont have the dropped columnns anymore, but here I want to show the original code and its original columns, and then show in another copy what can be done , to have a code without unecessary columns\n",
    "- The idea here is after we interpret the coefficients and notice coefficient of very low weights/ Odds ratio ~ equals 1. We can go back to the begining of the code and drop the unecessary columns and still get to the same results\n",
    "- We will drop the lowest weights features in the original data frame and then during standardization we also not include either them nor the binary that we also dont want standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using List Comprehension\n",
    "- 0) List comprehension is a syntactic construct which allows us to create a LIST from existing lists based on Loops, conditionals, etc\n",
    "- 1) We outline a list of all the columns names that we want to standardize \n",
    "    - That are included in the CURRENT input dataframe\n",
    "    - AND ALSO at the same time NOT INCLUDED in the columns to omit\n",
    "- We didnt STANDARDIZE NEITHER Binary variables NOR the Dropped variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_to_scale = ['Month',\n",
    "       #'Day of The Week', 'Transportation Expense', 'Distance to Work',\n",
    "       #'Age', 'Daily Work Load Average', 'Body Mass Index', \n",
    "       #'Children', 'Pets']\n",
    "\n",
    "# 1) First we outline the ones that are binary and we dont want to standardize\n",
    "\n",
    "# columns_to_omit=['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']\n",
    "\n",
    "# 2) Next we outline all the ones that we want to standardize included in the CURRENT columns and at the same time NOT INCLUDED in the columns to omit\n",
    "\n",
    "# columns_to_scale=[x for x in unscaled_inputs.columns.values if x not in columns_to_omit]\n",
    "\n",
    "# 3) this way we considered the columns we dont want standardized (binary and dummies) \n",
    "# and also considered the current most up to date dataframe which already had columns dropped from it that we also didnt need standardized\n",
    "\n",
    "# absenteeism_scaler = CustomScaler(columns_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Testing the Model - At the Test Data\n",
    "- Previous accuracy was Training Accuracy\n",
    "- THe important Accuracy is the Test Accuracy (usually lower)\n",
    "- If the test accuracy is lower than Train accuracy by 10-20% then overfitting happened otherwise not really\n",
    "\n",
    "#### Scenario 1: \n",
    "- Usually we test/evaluate new data the model HAS NOT seen before (TEST INPUT TEST TARGET) - Usually at the end of the machine learning process - SO ITS DONE ONCE AT THIS STAGE \n",
    "\n",
    "####  Scenario 2: \n",
    "- However Some researchers USE the test accuracy and then get back to the model to do tweaking and then check the test accuracy again - IN THIS CASE ITS CONSIDERED MANUAL TRAINING\n",
    "- We are training the model a bit more (Iterative process) - In this case the Test dataset is useless for the purpose of testing - you were training and not really testing once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dont run it more than once - if you do then start from begining so model has not seen it until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR SKLEARN - We can get the probability of the output being 0 or 1\n",
    "- .predict() gives the output\n",
    "- .predict_probaba() gives the probability of the output\n",
    "    - if probability is below 0.5 it makes the output 0, otherwise above 0.5 it makes it 1\n",
    "- Logistic regression calculates the probabilities in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba = reg.predict_proba(x_test)\n",
    "predicted_proba\n",
    "\n",
    "# first column is proba of being 0 second column probability of being 1 for the same observation - summing them up horizontally adds to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Probability of being absent is thus:\n",
    "- Since we care about the ones are 1s or absent  - SLICING IN NUMPY all rows second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_proba[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what now? NEXT STEPS\n",
    "- Save the model\n",
    "- Create a module for easier understading for other people\n",
    "- Get new data, classify it, pass it through SQL and analyze it in tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) PREPARING FOR DEPLOYMENT: \n",
    "## A) SAVING \n",
    "### LETS PICKLE?\n",
    "- We need to train the model >> Then save it >> Then load it in a new Notebook \n",
    "- In that saved file we will have (log regress, coeff,intercept, random shuffle used)\n",
    "- All that info was in the reg object - so we need to save the reg object into a file\n",
    "#### PICKLEI\n",
    "- it is the process of converting a python object into a character stream (will contain sufficient information) - later we can convert it back to python by UNPICKLING IT\n",
    "- So basically we will save the reg object into a file, the file is then loaded in a new notebook and thus we will be able to use the machine learning algorithm\n",
    "- file size is tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Reg object into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('My_model','wb') as file:\n",
    "    pickle.dump(reg,file)\n",
    "    \n",
    "# model is the file name\n",
    "# wb is write bytes - when unpickling we do rb read bytes\n",
    "# use the dump method - when pickling we dump the info into the file, when we unpickle we load it from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Scaler object into a file\n",
    "- Used to standardize all numerical variables AND it also stored the Mean and Standard Deviation of EACH FEATURE\n",
    "- So far our code was heavily depended on the train data, without the train data, the ML algo wouldnt execute at all \n",
    "\n",
    "- 1) Once we have trained the model we can seperate the model from the train data for GOOD\n",
    "- 2) We need the info stored inside scaler object of MEan and STD to standardize new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('My_scaler','wb') as file:\n",
    "    pickle.dump(absenteeism_scaler,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Side Note: Refreshing knowledge about CLASSES\n",
    "- A class has:\n",
    "    - Object (container) that can be initialized as an instance of that class - Once we have the object we can call out the attributes and the methods of the class\n",
    "    - Attributes (properties of that object)\n",
    "    - Method (action to apply to the object)\n",
    "    - Method Vs Function: \n",
    "        - Method belongs to a class object.method()\n",
    "        - Function exists on its own function()\n",
    "- Package/library is a collection of Modules \n",
    "    - Module is like what we can use for deployment (a collection of code for example to unpickle our model and data and can be deployed for new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) LOADING - WE LOAD VIA A MODULE\n",
    "- Now we need to load the pickle and unpickle it in another notebook...\n",
    "- THE HARD WAY: Taking new data and pass it through the same preprocessing code and adjusting all errors\n",
    "- THE EASIER WAY: Load the data UNPICKLE the Model file and the Scaler files using a MODULE\n",
    "\n",
    "### The Module (Module > Class > Methods > Attributes)\n",
    "- The module Contains: All the preprocessing code and the machine learning code\n",
    "- The Module Consists of 2 clases: custom scaler class and the model class\n",
    "- The model class consists of 5 methods:\n",
    "    - 1)  \"__INIT__\" Method called constructor. It initializes the attributes of the class\n",
    "        - It takes two arguments the model and the scaler files to unpickle them (convert them from files to objects again) model file >> reg, and scaler file >> scaler\n",
    "    - 2) Load and clean data: Same code as the preprocessing code used for the training \n",
    "    - 3) Predicted probability\n",
    "    - 4) Predicted Output Category\n",
    "    - 5) Predicted Outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) DEPLOYMENT/INTEGRATION... SEE NEXT FILES\n",
    "- We import the module - create an object an instance of the model class inside the module\n",
    "- Use the object with its attributes and methods\n",
    "- For Deployment you need:\n",
    "    - 1) 2 saved objects (model and scaler)\n",
    "    - 2) Module file containing and scaler class model class .py\n",
    "    - 3) csv containing new data to be fed into the model\n",
    "    - 4) A notebook to integrete everything by importing the module .ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
